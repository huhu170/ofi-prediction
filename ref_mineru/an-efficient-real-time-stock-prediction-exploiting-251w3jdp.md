# An efcient real‑time stock prediction exploiting incremental learning and deep learning

Tinku Singh1 $\textcircled{1}$ · Riya Kalra1 $\cdot$ Suryanshi Mishra2  · Satakshi2 $\cdot$ Manish Kumar1

Received: 6 September 2022 / Accepted: 13 December 2022   
$\circledcirc$ The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022

# Abstract

Intraday trading is popular among traders due to its ability to leverage price fuctuations in a short timeframe. For traders, real-time price predictions for the next few minutes can be benefcial for making strategies. Real-time prediction is challenging due to the stock market’s non-stationary, complex, noisy, chaotic, dynamic, volatile, and non-parametric nature. Machine learning models are considered efective for stock forecasting, yet, their hyperparameters need tuning with the latest market data to incorporate the market’s complexities. Usually, models are trained and tested in batches, which smooths the correction process and speeds up the learning. When making intraday stock predictions, the models should forecast for each instance in contrast to the whole batch and learn simultaneously to ensure high accuracy. In this paper, we propose a strategy based on two diferent learning approaches: incremental learning and Ofine–Online learning, to forecast the stock price using the real-time stream of the live market. In incremental learning, the model is updated continuously upon receiving the stock’s next instance from the live-stream, while in Ofine-Online learning, the model is retrained after each trading session to make sure it incorporates the latest data complexities. These methods were applied to univariate time-series (established from historical stock price) and multivariate time-series (considering historical stock price as well as technical indicators). Extensive experiments were performed on the eight most liquid stocks listed on the American NASDAQ and Indian NSE stock exchanges, respectively. The Ofine–Online models outperformed incremental models in terms of low forecasting error.

Keywords Real-time forecasting $\cdot$ Incremental learning $\cdot$ Technical indicator $\cdot$ Intraday trading

# 1 Introduction

Stock prices are afected by several micro and macro factors, such as the global economy, healthcare situation, oil prices, interest rates, news articles, public sentiment, etc. It is a signifcant task for fnancial companies to forecast stock prices, and rational forecasts can mitigate market risks and produce substantial returns. Several papers and studies have been devoted to making the best predictions and models possible based on the presence of many factors. The complexity of stock price prediction has made it a challenging problem, which has resulted in several papers and studies trying to make the most accurate predictions and models possible, owing to the massive potential for proft associated with them. In high-frequency trading, there is a large volume of orders, proprietary trading, and a short retention period, according to the Securities and Exchange Commission (Menkveld 2013). According to Aldridge and Krawciw, the most typical deal in 2016 started at $1 0 \% { - } 4 0 \%$ of trading volume and $1 0 \% { - } 1 5 \%$ of exchange rate and assets (Aldridge and Krawciw 2017). The frequency of performance tasks in the stock market has escalated to a fraction of a second due to the enormous expansion of the internet (Bagheri et al. 2014). High-frequency trading is currently a very popular form of trading whose sole aim is to maximize profts by buying and selling stocks in a short span. Data patterns may be more valuable than sentiments and news articles. Since most of the news surrounding stocks is not generated regularly, only those posted on Twitter and other outlets can be accessed. Because of the difculty of analyzing so many factors simultaneously, this study focuses on high-frequency data, i.e., short-interval stock prices, to predict the current price.

Since stock market data is generated periodically, it is considered time-series data. Stock market data is a series of time-ordered data points associated with single or multiple time-dependent variables. It has local and global patterns produced by the movements of prices on a chart and is the basis of technical analysis. Time-series can be classifed as univariate or multivariate. Univariate time-series models only have one dependent variable, whereas multivariate models consider multiple factors. Training a univariate timeseries model simply relies on past price movements. While the current stock price is afected by many factors, such as the closing or opening price, univariate predictive models reduce this complexity to a single factor and ignore all other dimensions. Multivariate time-series forecasting models take into account a variety of factors, such as the relationship between closing and opening prices, various technical indicators, daily highs and lows, and moving averages. When dealing with stock market data, several time-series components like trends, periodic swings, seasonal patterns, and random volatility might contribute to improved stock price forecasting. The trends are the result of long-term efects and can increase or decrease the time-series value over time. Periodic swings occur over the length of a time-series and are aimed at capturing short to medium-term gains in stock prices. Irregular movements exhibit rapid changes in timeseries that are difcult to repeat, such as COVID’19.

Forecasting stock market trends based on live-streaming data has been a challenge for fnancial analysts and researchers. A streaming data process difers from traditional processing tools, which store and process data in batches. Stock prediction is one of the most widely used applications that require the real-time processing of streaming data. However, making decisions is challenging due to the market’s complexity and chaotic dynamics, as well as the numerous nonstationary, undecidable, and unpredictable factors involved. The need to estimate the domestic stock market in several countries makes accurate forecasting even more challenging because there are various cultures, traditions, and diverse sources that may impact investors’ decision-making processes. Based on previous trends in fnancial time-series, professionals from diverse sectors have created numerous forecasting methodologies. To achieve promising performance, most of these methods require careful selection of the input variables, the establishment of a predictive model coupled with professional fnancial knowledge, and the use of various statistical methods. As a result, it is difcult for people outside of the fnancial industry to estimate stock values using such approaches. The fuctuating nature of data and the heterogeneity of data types makes forecasting more complex based on technical analysis. Our main objective is to devise a lightweight prediction for the number of companies with fair accuracy, useful enough for intraday trading.

The objective of this study is to predict the closing price of stocks for the coming 15 mins utilizing the current stock price extracted through streaming data along with technical indicators calculated through this data to improve accuracy. The idea is to train a model using high-frequency historical stock market data at short intervals and then apply it in real-time. It is difcult to spot a trend over a short period, such as $1 \ \mathrm { m i n }$ or 5 mins because there is a lot of noise. With a longer time frame, such as 15 mins, it can be easier to identify patterns, support, and resistance. Therefore, in order to get more reliable outcomes, we will use the data stream of a $1 5 \mathrm { - m i n }$ interval for real-time forecasting. Two diferent approaches have been adopted for the study: incremental learning, where the model will update with every single collected current stock price from the data stream, and Ofine–Online, where the model is retrained at the end of every trading session. Incremental linear regression has been utilized for incremental models, while the variants of LSTM and CNN have been adopted for forecasting through Ofine–Online models. In the Ofine–Online approach, Ofine involves analyzing a batch of data and optimizing the model to make a prediction, whereas Online refers to taking samples from the streaming data and making the prediction. However, incremental learning targets building a learning model that adapts to new data without losing any existing knowledge. In the Ofine–Online approach, the model is not fne-tuned on receiving every new instance from the stream, although it is tuned after each trading session since the stock market might be impacted by multiple factors throughout a session. The incremental learning model updates with each stream instance, so it does not require retraining with the entire dataset.

This paper makes the following contributions:

• Stock prices are transformed from a univariate to a multivariate time-series with technical indicators, which allows for better forecasting. The paper utilizes deep learning models in real-time forecasting, which has been achieved through the model training after the entire trading session, rather than after retrieving the next stock instance, while real-time lag val-

ues and technical indicators of the stocks are maintained using local variables.   
Ofine-Online and incremental learning approaches are compared for real-time forecasting.   
It empirically demonstrates the performance of the proposed system on the eight most liquid stocks of the NAS-DAQ and NSE, respectively, for one year.

# 2 Literature review

A stock market forecast involves predicting the future movement of a stock’s value on a fnancial exchange. The efcient forecasting of share prices ofers investors great proft potential, and correctly predicting the price movement within a short span can result in substantial profts. Several methods have been proposed for forecasting the market and providing decision-making guidance. Stock prices, rather than being stochastic, can be viewed as discrete time-series that are based on well-defned numbers collected at regular intervals of time. To build the forecasting model, the time-series data must be stationary. The diferencing approach can be applied to obtain stationary data from a non-stationary time-series. On the other hand, the trend information in the time-series will be ignored by the diferencing technique. Diferent methods can be applied in this area, including statistical methods and machine learning models. Generally, statistical models assume that there is a linear correlation structure among the time-series values. However, the nature of the stock market time-series is non-linear, volatile, chaotic and highly noisy (Alves et al. 2018). The autoregressive method (AR), the moving average model (MA), the combination of both AR and MA, i.e., the autoregressive moving average model (ARMA), and the autoregressive integrated moving average (ARIMA) are all traditional statistical methods. The ARIMA model’s popularity originates from its statistical features as well as the notable Box-Jenkins model-building methodology. However, ARIMA models are not able to capture nonlinear patterns, and resembling complex real-life problems with linear models is not always practical (Zhang 2003). The researchers proposed the Grader causality test, which elongates the analysis from a univariate to a multivariate time-series analysis. Using the vector autoregressive moving average (VARMA), a multivariate time-series forecasting model was developed, which can represent Vector Moving Average (VMA) and Vector Autoregressive (VAR) models fexibly (Liu et al. 2021). Using a generalized autoregressive conditional heteroscedastic (GARCH) model for conditional variances, Pellegrini et al. (2011) apply the ARIMA-GARCH model to the forecasting of a fnancial series. Since the ARIMA-GARCH models never converge to homoscedastic intervals, their prediction intervals may be inadequate.

Traditional time-series forecasting algorithms can capture linear correlations and yield good results for a small dataset. But these algorithms are not very efective when used for time-series that are large and complex, such as stock market time-series (Liu et al. 2021). As a result, researchers focused increasingly on machine learning and deep learning methods in this domain. Javed Awan et al. (2021) utilized machine learning algorithms and sentiment analysis for forecasting stock prices. As per the outcomes, linear regression, extended linear regression, and random forest produce more accurate outcomes than the decision tree. Several studies have used linear and non-linear support vector machines (SVMs) for the forecasting of fnancial time-series (Cao and Tay 2001; Kim 2003; Maguluri and Ragupathy 2020). However, overftting is a problem with these models, and the algorithms are not good at predicting large datasets. As compared to other models, support vector regression has better accuracy, according to Behera et al. (2020). Tuarob et al. (2021) created an end-to-end framework containing three sub-models, i.e., Davis-C for data collection related to stocks in real-time, Davis-A for analysis, and Davis-V for visualization. Their framework demonstrates that a combination of machine learning algorithms outperforms a standalone machine learning algorithm by large margins. Vijh et al. (2020) developed two models: one that predicts the price trends for the next day using historical data, and another that predicts the price trends for the next month using historical data. They employed Logistic Regression, SVM, and Boosted Decision Tree to forecast the trend based on volume volatility, sentiment, and continuous up/down.

In recent years, deep learning methods have become increasingly popular for predicting stock market moves. From complex and inconsistent data, these approaches can extract signifcant characteristics and detect underlying nonlinearities without relying on human skill (Kumar et al. 2021). Several experts have used deep learning to improve stock forecasting and produce profts for shareholders. In fnancial time-series forecasting, deep learning methods like artifcial neural networks (ANN), convolutional neural networks (CNN), long-short-term memory (LSTM), hybrid algorithms, and others lead to better outcomes than statistical and machine learning methods. Vijh et al. (2020) explored the ANN and Random Forest on multivariate time-series on fve stocks to forecast the next day’s closing price using features such as the previous day’s open price, closing price, Moving Average, Highs, and Lows. Lu et al. (2020) proposed a hybrid CNN-LSTM stock forecasting method. The authors compared the suggested model’s performance to that of MLP, CNN, RNN, LSTM, and CNN-RNN on the Shanghai Composite Index. According to the experimental fndings, the CNN-LSTM came up with the most accurate stock price forecasting, with an MAE of 27.564 and an RMSE of 39.688. Wen et al. (2020)

utilized the PCA-LSTM, which used the PCA (Principal Component Analysis) technique to identify technical indicator features and decrease dimensionality, yielding more accurate forecasts. DJI. Ince and Trafalis (2008) focused on short-term forecasts and used the SVM model in stock price forecasts. Specifcally, their main contribution consists of comparing MLPs with SVMs and fnding conditions where SVM is more efective than MLP. Moreover, diferent trading strategies afect the results. They contribute primarily by comparing MLP and SVM and fnding cases in which SVM works better than MLP. Moreover, diferent trading strategies also afect the results. Dan et al. (2014) demonstrated the forecasting capabilities of deterministic Echo State Networks (ESNs) in stock prediction applications. Their experiments with the S & P 500 dataset show that the deterministic ESNs have improved their efciency by about $2 3 \%$ compared to the standard ESN while demonstrating a negligible gain in predicting accuracy. Li et al. (2022) presented an efective deep learning-based BiGRU-attention model for short-term voltage stability assessment. It extracts temporal relationships and performs well even with a limited training dataset.

Intraday traders work with minute-based or sometimes even second-based stock market data. As a result, it is very crucial to determine how to analyze useful information and identify whether the forecasting method can be efective in real-time on high-frequency stock market data. Shakva et al. (2018), utilized ANN to predict stock prices on the Nepal Stock Exchange. The authors tried to predict the percentage increase or decrease in stock prices every second minute. They used technical indicators along with the data from the past $3 0 \mathrm { { m i n } }$ . Selvamuthu et al. (2019) proposed the use of Levenberg-Marquardt, Scaled Conjugate Gradient, and Bayesian Regularization algorithms for predicting stock prices on a common ANN architecture of 20 hidden layers. They used the high-frequency dataset of Reliance Private Ltd. from Thomson Reuter over one year with 15,000 data points per day and were able to obtain a MAPE of $9 9 . 9 \%$ using tick data and $9 8 . 9 \%$ over a 15-minute dataset. Zhou et al. (2018) present a generic framework for adversarial training to anticipate the high-frequency stock market using LSTM and CNN. To avoid complex fnancial theory research and challenging technical analysis, this model employs a publicly available index ofered by trading software as an input, which makes it more suitable for the typical nonfnancial trader. Liu et al. (2021) suggest a general framework for automatically developing a high-frequency trading strategy using a PPO-based agent. The study compares the LSTM and MLP for price prediction based on bitcoin prices in real-time. The study demonstrates the efectiveness of a PPO-based LSTM agent over an MLP, which earns high returns even when the market is in a slump and the price fuctuates.

According to the literature survey, most of the papers only forecast using historical data and do not operate with real-time data. The majority of them utilize historical dayto-day closing prices rather than current stock prices and do not deal with short time intervals such as fve minutes or ffteen minutes. Stock market data is highly volatile and produced in massive amounts, making it difcult to manage and much more difcult to forecast. A majority of the studies used univariate stock market forecasting models, which do not take advantage of the technical indicators and other infuential features to improve their accuracy. To leverage the advantages of technical indicators, we have converted the univariate stock series to a multivariate series. Deep learning models are efective in stock forecasting but have limitations like complex model training and a long training time, which makes it challenging to train the model in real-time on the new stock instances. The motivation of this research is to use deep learning models in real-time to forecast high-frequency stock data and to leverage the advantages of technical indicators by converting the univariate stock series to a multivariate series. The system in this paper aims to fll the void left by existing models for high-frequency trading.

# 3 Dataset

The forecasting would be centered around high-volume stocks since intraday traders tend to be most interested in them because of buyers’ and sellers’ availability throughout the trading session. For this study, we selected fnancial time-series (stocks) from the Indian and U.S. stock markets. The Bombay Stock Exchange (BSE) and the National Stock Exchange (NSE) are India’s two major stock exchanges. We selected NSE because the volumes traded there are far higher than those traded on BSE. NIFTY 50 is an index of the top 50 companies listed on the NSE; we considered the top eight Nifty-50 stocks (India NSE 2001). There are two major stock exchanges in the United States: NASDAQ and the New York Stock Exchange (NYSE). Due to the NASDAQ’s volatility and the number of listed companies, we will be using NASDAQ-traded stocks instead of NYSE stocks. According to market capitalization, the NASDAQ-100 is an index of the top 100 publicly traded companies, so eight of the most traded stocks were selected for the study (Nasdaq 2022). The selected stock from both exchanges can be seen in Table 1.

The high-frequency historical data of 15-min time intervals and live feeds of NSE stocks have been extracted using web-scraping through Zerodha API [27]. AlphaVantage API provided both real-time as well as historical stock prices for NASDAQ stocks [28]. A snapshot of the live-streamed stock prices is presented in Fig. 1.

Table 1 Selected stocks for study   

<table><tr><td>S. No.</td><td>NSE stocks</td><td>NASDAQ Stocks</td></tr><tr><td>1</td><td>RELIANCE INDUSTRIES (RELIANCE)</td><td>APPLE (AAPL)</td></tr><tr><td>2</td><td>HDFC BANK (HDFCBANK)</td><td>MICROSOFT (MSFT)</td></tr><tr><td>3</td><td>INFOSYS (INFY)</td><td>AMAZON (AMZN)</td></tr><tr><td>4</td><td>ICICI BANK (ICICIBANK)</td><td>GOOGLE (GOOGL)</td></tr><tr><td>5</td><td>HDFC (HDFC)</td><td>NVIDIA (NVDA)</td></tr><tr><td>6</td><td>TATA CONSULTANCY SERVICES (TCS)</td><td>TESLA (TSLA)</td></tr><tr><td>7</td><td>KOTAK MAHINDRA BANK (KOTAKBANK)</td><td>BERKSHIRE HATH-</td></tr><tr><td>8</td><td>ITC LTD. (ITC)</td><td>WAY (BRK-B) FACEBOOK (FB)</td></tr></table>

"2022-02-28 14:40:00": { "2022-02-28 14:35:00": { "1. open": "121.1050", "1. open": "121.0998", "2. high": "121.3800", "2. high": "121.2000", "3. low": "121.1050", "3. low": "121.0388", "4. close": "121.2450", "4. close": "121.1050", "5. volume": "38987" "5. volume": "27560"   
}\$ },\$

# 4 Methodology

This study analyzes time-series forecasting models for efcient forecasting of stock prices utilizing high-frequency data (15-min intervals). The proposed approach is based on two diferent learning methods: incremental learning and Ofine-Online learning. These methods are applied to univariate and multivariate time-series. The univariate time-series was created using stock prices, whereas the multivariate time-series was created using stock prices in conjunction with exponential moving averages (EMAs) and volume-weighted average prices (VWAPs). The incremental model is continuously updated as it receives new instances of the stock price from the live feed of the stock market. On the other hand, the Ofine–Online learning model needs to be retrained after every trading session. A retraining of the model will enable it to adapt to the current market trends, volatility, and seasonality. Figure 2 displays a visual representation of the methodology.

A pre-processing step is required before the model can be adapted for forecasting. Preprocessing includes removing null values and duplicate instances, verifying the order of instances, and fnally converting the string date-time value containing UTC into a numerical timestamp. To perform the forecasting of the high-frequency stock market data, it is mathematically suitable to consider the time-series analysis with this condition $\{ Y _ { t } | t \in T \}$ . A special type of examining stock prices (sequence of instances) collected over an interval of time is known as a “time-series analysis” (González et al. 2017). A time-series process $\{ Y _ { t } | t \in T \}$ , is a stochastic process in which a set of random variables is ordered through time. T stands for index sets, which are distinct and separated evenly in time. Random variable $Y _ { t }$ is continuous. Let $i \in \mathbb { N } , T \subseteq \mathbb { R }$ . A function $y : T  \mathbb { R } ^ { i }$ , $t  y _ { t }$ or, similarly, a set of indexed elements of $\mathbb { R } ^ { i }$ ,

![](images/d5274dbfe2a33d9b8a4aef7c50c88e6c16623f755281e20baf2dc9e2b1060de2.jpg)  
Fig. 1 Snapshot of Live stream format of extracted data   
Fig. 2 Proposed model

$$
\{ y _ { t } | y _ { t } \in \mathbb { R } ^ { i } , t \in T \}
$$

is called an observed time-series. It can also written as: $y _ { t } ( t \in T )$ or $( y _ { t } ) _ { t \in T }$ .

The variance function (fuctuation) of a time-series process $( X _ { t } )$ is defned as if $\forall t \in T$ :

$$
\sigma _ { t } ^ { 2 } = V a r [ Y _ { t } ] , \sigma _ { t } ^ { 2 } = E [ Y _ { t } ^ { 2 } ] - E [ Y _ { t } ] ^ { 2 } , \forall t \in T
$$

For historical stock data, if we assume that the mean and variance are constant then, $\mu _ { t } = \mu$ and $\sigma _ { t } ^ { 2 } = \sigma _ { t }$ .

Therefore, the obvious estimate is:

$$
{ \hat { \mu } } = { \frac { 1 } { n } } \sum _ { t = 1 } ^ { n } Y _ { t } ; { \hat { \sigma } } ^ { 2 } = { \frac { 1 } { n - 1 } } \sum _ { t = 1 } ^ { n } ( Y _ { t } - \mu ) ^ { 2 }
$$

A candlestick chart of the time-series for TCS over a 15-minute interval can be seen in Fig. 3. The stock prices are not generated at random but rather as a discrete-timeseries created by collecting the numerical values at regular intervals. A candlestick shows the open, high, low, and close prices over an interval. Red candles represent the current closing price being higher than the previous candle’s closing price, while green candles represent the lower price.

# 4.1 Technical indicators

In order to be efective in stock price prediction, traders utilize technical charts by analyzing price actions and technical indicators. There are numerous technical indicators that intraday traders use to determine when to buy or sell a particular stock, such as MACD and RSI. However, to capture current trends, $E M A ( d )$ and VWAP are both suitable indicators. $E M A ( d )$ is the average price of the stock in the previous $d$ data points weighted exponentially (this way the prices of recent data points are given more weight). Since EMAs focus on recent price movements, they tend to respond more quickly to price changes. When trading intraday, it is considered reliable to use the value 5 to 20 for $d$ in $E M A ( d )$ . It can be calculated as:

$$
E M A _ { t } = \alpha \cdot Y _ { t } + ( 1 - \alpha ) \cdot E M A _ { ( t - 1 ) }
$$

where $E M A _ { t } , Y _ { t }$ denotes the EMA and closing price respectively at time $t$ , and $\alpha$ is a smoothness coefcient between 0 and 1 that denotes the degree of weight reduction. For $d$ previous observations $\alpha$ can be computed as:

$$
\alpha = 2 / ( d + 1 )
$$

VWAP indicates the average price of the stock being traded in a day based on both price and volume. It is the ratio between the stock value and volume traded in a specifc period. The indicator only works for one trading session and resets at the beginning of the next trading session. Suppose (Chen et al. 2013) that we have a large order v that must be executed during a specifed time interval T. In that case, we must slice it into several smaller orders $\nu _ { i }$ and trade them over the time interval $i$ from $t _ { i - 1 }$ to $t _ { i } ( t _ { i } = i * L + t _ { 0 }$ , where $L$ is the length of each time interval and $t _ { 0 }$ is the start time for the trade).

![](images/7ee8e7f1f136152651cba4da9c4f2f7f6267814fbc974975505dc92c061fc2d9.jpg)  
Fig. 3 Time-series of Stock Price

$$
V W A P _ { S } = \frac { \sum _ { i = 1 } ^ { n } { p _ { i } * \nu _ { i } } } { \nu }
$$

$n = L / T$ denotes the trading periods, while $\textstyle \nu = \sum _ { i = 1 } ^ { n } \nu _ { i }$ represents the time interval. To generate the multivariate time-series, the EMA(10) and VWAP are calculated using historical stock data, and the resultant series are combined with the stock price to get the fnal series.

# 4.2 Correlation/covariance analysis

The covariance and correlation functions defne the level of dependency between the variables for any stock instances

(random variables) $X _ { p }$ and $X _ { q }$ . Auto-covariance function (ACVF) of the time-series $\{ X _ { p } , X _ { q } | p , q \in T \}$ is defned as,

$$
\begin{array} { c } { { C o \nu [ X _ { p } , X _ { q } ] = E [ ( X _ { p } - E [ X _ { p } ] ) ( X _ { q } - E [ X _ { q } ] ) ] } } \\ { { { } } } \\ { { \gamma _ { p , q } = C o \nu [ X _ { p } , X _ { q } ] = E [ X _ { p } X _ { q } ] - E [ X _ { p } ] E [ X _ { q } ] } } \end{array}
$$

$\gamma _ { p , q }$ ∶ Auto-covariance function of the given time-series.

The auto-correlation function (ACF) for the stochastic process is defned as:

$$
C o r r [ X _ { p } , X _ { q } ] = \frac { C o \nu [ X _ { p } , X _ { q } ] } { \sqrt { V a r [ X _ { p } ] V a r [ X _ { q } ] } }
$$

For any two sets of stock instances $( r _ { 1 } , r _ { 2 } , . . . , r _ { n } )$ and $( s _ { 1 } , s _ { 2 } , . . . . . , s _ { n } )$ , the sample of covariance and correlation functions are given as:

$$
\hat { \gamma } _ { r , s } = \frac { 1 } { n - 1 } \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) ( s _ { t } - \bar { s } )
$$

$$
{ \hat { \rho } } _ { r , s } = { \frac { \sum _ { t = 1 } ^ { n } ( r _ { t } - { \bar { r } } ) ( s _ { t } - { \bar { s } } ) } { { \sqrt { \sum _ { t = 1 } ^ { n } \left( r _ { t } - { \bar { r } } \right) ^ { 2 } \sum _ { t = 1 } ^ { n } \left( s _ { t } - { \bar { s } } \right) ^ { 2 } } } } }
$$

$\hat { \rho } _ { r , s }$ ∶ Auto-correlation function of the stochastic process. However, for time-series data the ACVF and ACF measure the covariance/correlation between the single time-series $( r _ { 1 } , r _ { 2 } , . . . , r _ { n } )$ and itself at diferent lags.

Using Eq. 1 & 2 at lag 0, the $\mathrm { A C V F } \hat { \gamma } _ { 0 }$ , is the covariance of $( r _ { 1 } , r _ { 2 } , . . . , r _ { n } )$ with $( r _ { 1 } , r _ { 2 } , . . . , r _ { n } )$ (or same series) and itself then,

$$
\begin{array} { l } { { \hat { \gamma } _ { \boldsymbol { 0 } } { } = { } \displaystyle \frac { 1 } { n - 1 } \sum _ { t = 1 } ^ { n } ( r _ { t } - { \bar { r } } ) ( r _ { t } - { \bar { r } } ) } } \\ { { \hat { \gamma } _ { \boldsymbol { 0 } } { } = { } \displaystyle \frac { 1 } { n - 1 } \sum _ { t = 1 } ^ { n } ( r _ { t } - { \bar { r } } ) ^ { 2 } } } \end{array}
$$

Similarly, the $\mathrm { A C F } \hat { \rho } _ { 0 }$ , the correlation lies itself then,

$$
\begin{array} { r l } & { \hat { \rho } _ { 0 } = \frac { \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) ( r _ { t } - \bar { r } ) } { \sqrt { \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) ^ { 2 } \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) ^ { 2 } } } } \\ & { \hat { \rho } _ { 0 } = \frac { \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) ( r _ { t } - \bar { r } ) } { \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) \sum _ { t = 1 } ^ { n } ( r _ { t } - \bar { r } ) } = 1 } \end{array}
$$

The auto-correlation function (ACF) $\&$ partial auto-correlation function (PACF) can be utilized to describe the order of stock price movements [31]. Let $Y _ { t }$ be the stationary timeseries and $Y _ { t - h }$ with the lagged value of h. PACF estimates the degree of correlation between $Y _ { t }$ and $Y _ { t - h }$ but ignores the other time lags. We can predict $x$ and $y _ { 3 }$ with the help of $y _ { 1 }$ and $y _ { 2 }$ variables:

$$
\frac { C o \nu ( x , y _ { 3 } | y _ { 1 } , y _ { 2 } ) } { \sqrt { V a r ( x | y _ { 1 } , y _ { 2 } ) V a r ( y _ { 3 } | y _ { 1 } , y _ { 2 } ) } }
$$

Here, $y _ { 1 } , y _ { 2 }$ , and $y _ { 3 }$ are the regression coefcients. In regression, $x$ is a response variable, while the predictor variables are $y _ { 1 } , y _ { 2 }$ , and $y _ { 3 }$ . A partial correlation exists between $\mathbf { X }$ and $y _ { 3 }$ , describing their association with $y _ { 1 }$ and $y _ { 2 }$ and indicating how dependent they are on one another. We defne frst-order with partial auto-correlation as being equal to frst-order auto-correlation.

Based on Fig. 4 it can be observed that lag values of $1 5 \mathrm { { m i n } }$ before at position 1 have a strong positive correlation with the current observations. In all three features, VWAP, Price, and EMA, the correlation is strong up to the lag value of 3 or up to $4 5 \mathrm { { m i n } }$ , but beyond that, the correlation is not signifcant. Based on the analysis, a maximum of three lags are required for reliable forecasting. Furthermore, the lag values (3, 9, 27) were tested for reliability and consistency with the models in this study, and lag 3 was found to be reasonable in most scenarios.

# 4.3 Incremental approach

A stock price forecast is frst derived through an incremental model. It uses incremental linear regression to predict the stock price for the next interval. Once the actual price for the next instance is captured through the data stream, it will estimate the prediction accuracy and update the model accordingly. The machine learning technique of incremental learning extends the existing model’s knowledge by continuously using input data, i.e., by further training, it (Iscen et al. 2020). The ordered pair of $( y _ { j } , z _ { j } )$ is denoted by the $j ^ { t h }$ pair of input and output observations. In the stock market, the correct output is considered to be $\mathcal { F } ( y _ { j } )$ if the system has provided data specifed by a function $\mathcal { F }$ . As a consequence of systematic noise or measurement error, the measured output $z _ { j }$ is consistent with $z _ { j } = \mathcal { F } ( y _ { j } ) + \epsilon _ { j }$ , where $\epsilon _ { j }$ is inevitable, but hopefully, it is the minor term. If the function $\mathcal { F }$ has a $m ^ { t h }$ pair of observations, these ordered pairs are: $\{ ( y _ { 1 } , z _ { 1 } ) , ( y _ { 2 } , z _ { 2 } ) , . . . , ( y _ { m } , z _ { m } ) \}$ . Even if we use $\mathcal { F } ( \boldsymbol { y } )$ to estimate z for an unobserved y, it will defne a loss function $\mathcal { L } ( \boldsymbol { z } , \mathcal { F } ( \boldsymbol { y } ) )$ to evaluate the error which will occur. New observations that occur outside of our training set are classifed as unobserved y. Here, the loss functions of the target function are $\mathcal { F }$ .

Due to the linear regression, a linear function of the input vector is $\mathcal { F } ( y ) = W ^ { T } y$ . Assume the loss function of the loss squared function is:

$$
\mathcal { L } ( \boldsymbol { z } , W ^ { T } \boldsymbol { y } ) = ( \boldsymbol { z } - W ^ { T } \boldsymbol { y } ) ^ { 2 }
$$

Therefore, the gradient of $\mathcal { L }$ with regard to a weight vector is defned as:

![](images/af7bbb6e9560872aede0aac851e380b65174968b3a4849a335f11eb71cc0dd15.jpg)  
Fig. 4 Partial auto-correlation for VWAP, price and EMA

$$
\nabla _ { \scriptscriptstyle W } \mathcal { L } = - 2 ( z - W ^ { T } y ) y
$$

Since the gradient represents the increased direction of the function, if we want the squared loss to decrease, we have to move the weight vector in the opposite direction of the gradient. We get the $t ^ { t h }$ observation $y _ { t }$ at time t, and we may estimate the outcome as follows:

$$
\hat { z _ { t } } = W _ { t - 1 } ^ { T } y _ { t }
$$

Updated estimate of $W$ is defned as:

$$
W _ { t } = W _ { t - 1 } + \rho _ { t } ( z _ { t } + \hat { z } _ { t } ) y _ { t }
$$

where $\rho _ { t } > 0$ is known as step size. The step size is given as

$$
\rho _ { t } = { \frac { \rho _ { o } } { \sqrt { t } } }
$$

for some predefned constant $\rho _ { o }$ . The cumulative regret after t steps provides as a metric of efectiveness, which is defned as:

$$
R e g r e t = \sum _ { t = 1 } ^ { T } ( \boldsymbol { z } _ { t } - \boldsymbol { \hat { z } } _ { t } ) - \sum _ { t = 1 } ^ { T } ( \boldsymbol { z } _ { t } - \boldsymbol { W } _ { t } ^ { T } \boldsymbol { y } _ { t } ) ^ { 2 }
$$

Eq.(3), which is utilized in a fnancial decision-making system, where $W ^ { T } y _ { t }$ is the optimum at step t, and the regret quantifes the total losses due to the non-optimal decisions.

# 4.4 Ofine–online approach

For the Ofine–online approach, fnancial time-series need to be converted into supervised learning problems to train a model. Since the model learns a function that maps a sequence of past observations as input to an output observation while it’s being trained. That’s why the dataset has to be prepared in the form of input samples. Each sample will take the current timestamp observation as the target value with n number of the previous instance as features where n is called lag observations. The model is trained after every trading session, and checkpoints are created for the trained model. The checkpoints help in storing the model’s architecture, weights, and training confguration in a single fle. Since the optimizer state of the model is recovered, it does not require retraining, and training can be resumed from the point at which it was stopped. A wide range of deep learning models for efective time-series forecasting have been utilized in this study, including LSTM and its variants; vanilla, stacked, and bi-directional LSTM, CNN, and CNN-LSTM.

LSTM is a type of artifcial neural network (ANN) that excels at classifcation and regression tasks. LSTM (Graves et al. 2005) is a special kind of recurrent neural network (RNN) capable of handling long-term dependencies. The LSTM network is an advanced RNN, a sequential network, that allows information to persist. B-LSTM model is based on the bidirectional RNN model, which passes the information (Rathor and Agrawal 2021). It gives any neural network the ability to store the data backward or forward in both directions, at the same time. We can also have input fow in both directions, allowing us to save both previous and current data at any time step. These equations represent the forward $(  )$ process as follows:

$$
\begin{array} { r l } & { \overline { { F _ { t } } } = \overrightarrow { \sigma } ( \overline { { W _ { f } } } * \overline { { X _ { t } } } + \overline { { V _ { f } } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { f } } } ) } \\ & { \overline { { I _ { t } } } = \overrightarrow { \sigma } ( \overline { { W _ { i } } } * \overline { { X _ { t } } } + \overline { { V _ { i } } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { i } } } ) } \\ & { \overline { { O _ { t } } } = \overrightarrow { \sigma } ( \overline { { W _ { o } } } * \overline { { X _ { t } } } + \overline { { V _ { o } } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { o } } } ) } \\ & { \overline { { C _ { t } ^ { \prime } } } = t a n h ( \overline { { W _ { c } } } * \overline { { X _ { t } } } + \overline { { V _ { c } } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { c } } } ) } \\ & { \overline { { C _ { t } } } = \overline { { F _ { t } } } * C _ { t - 1 } + \overline { { I _ { t } } } * \overline { { C _ { t } ^ { \prime } } } } \\ & { \overline { { h _ { t } } } = \overline { { O _ { t } } } * t a n h ( \overline { { C _ { t } } } ) } \end{array}
$$

In the backward $(  )$ process, there are some equations as follows:

$$
\begin{array} { r l } & { \overline { { F _ { t } } } = \overleftarrow { \sigma } ( \overline { { W _ { f } } } * \overline { { X _ { t } } } + \overleftarrow { V _ { f } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { f } } } ) } \\ & { \overline { { I _ { t } } } = \overleftarrow { \sigma } ( \overline { { W _ { i } } } * \overline { { X _ { t } } } + \overline { { V _ { i } } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { i } } } ) } \\ & { \overline { { O _ { t } } } = \overleftarrow { \sigma } ( \overline { { W _ { o } } } * \overline { { X _ { t } } } + \overleftarrow { V _ { o } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { o } } } ) } \\ & { \overline { { C _ { t } ^ { \prime } } } = t a n h ( \overline { { W _ { c } } } * \overline { { X _ { t } } } + \overleftarrow { V _ { c } } * \overline { { h _ { t - 1 } } } + \overline { { Z _ { c } } } ) } \\ & { \overline { { C _ { t } } } = \overleftarrow { F _ { t } } * C _ { t - 1 } + \overleftarrow { I _ { t } } * \overleftarrow { C _ { t } ^ { \prime } } } \\ & { \overline { { h _ { t } } } = \overleftarrow { O } * t a n h ( \overline { { C _ { t } } } ) } \end{array}
$$

Where, $\overleftarrow { F } _ { t } , \overleftarrow { I } _ { t }$ , and $\overleftarrow { O _ { t } }$ represent the backward forget input and output gate of the B-LSTM model. A weight matrix associates $\underbrace { \overleftarrow { W _ { f } } } _ { \overleftarrow { W _ { i } } } , \overleftarrow { W _ { o } }$ , and $\overleftarrow { W _ { c } }$ with the inputs $\bar { \overline { { X } } } _ { t }$ . Here $\overleftarrow { Z _ { f } } , \overleftarrow { Z _ { i } } , \overleftarrow { Z _ { o } }$ , and $\overleftrightarrow { Z _ { c } }$ are the biased functions of the backward process model. The $\overleftarrow { \boldsymbol { \sigma } }$ , and tanh are the sigmoid and activation function of the model, respectively. $\bar { \overline { { h _ { t } } } }$ is the hidden state of the current timestamp, and $\overleftarrow { h _ { t - 1 } }$ is the hidden state of the previous timestamp of the B-LSTM model. In the same way, the B-LSTM forward process works.

A vanilla LSTM (V-LSTM) (Wu et al. 2018) consists of an LSTM model with a single hidden layer and an output layer for the prediction. It can separate the efects of a performing variant change. In V-LSTMs there is a forget gate, allowing continuous learning. They also train using gradients rather than weight portions, as ESNs do.

For complex sequence classifcation challenges, stacked LSTM (Du et al. 2017) has become a reliable approach. An LSTM model with stacked layers can be called stacked LSTM (S-LSTM) architecture. When there is a long-term range between the data or a multivariate time dataset, connecting with several LSTM layers enhances the forecasting performance. Based on Fig. 5, $X _ { t }$ transmits the LSTM-1 layer with the hidden state $h _ { t - 1 }$ as the input vector and exists as $h _ { t }$ as the output vector, while $h _ { t }$ is the input vector for the LSTM-2 layer. The ultimate output, $h _ { t } ^ { m }$ , is generated when all of the LSTM- $\mathbf { \nabla \cdot m }$ layers have been stacked.

Figure 7 represents a hybrid CNN-LSTM deep learning model that is assessed to estimate stock prices, combining the benefts of both the CNN and LSTM models.

The temporal dependencies are contained in the current input data and trained by the hybrid LSTM model. Figure 6 shows the architecture of the LSTM when the CNN input vector $Y$ is input and the predicted data $Z$ is output. The CNN system is integrated in such a way that it can handle multidimensional data. The information received in the input layer consists of various stock price sequences $\{ T _ { 1 } , T _ { 2 } , . . . , T _ { r } \}$ which are mainly composed of the indicators dataset. Due to convolution and pooling layers, $r$ convolution layers for each stock data have been used to produce $r$ feature maps from the indicator dataset and to generate $r$ feature vectors, which will also be referred to as an $r$ channel. Every feature vector is fused in the matrix $X _ { T _ { r } }$ as describes:

$$
X _ { T _ { r } } ^ { d a t a s e t } = R e L U ( d a t a s e t , T _ { r } )
$$

This convolution layer consists of a flter $W _ { c } \in R ^ { g \times h }$ , where $g$ represents the dimension and $h$ represents the step size in the feature vectors. As a consequence of the flter, the following feature vector is generated (Ren et al. 2015):

![](images/53ae6c206bfdb4042006aef07e07207cd28a158a5a6c5c3f5d6faf1c6ae6265d.jpg)  
Fig. 5 Architecture of stacked LSTM

$$
c = F ( C o n \nu ( X _ { T _ { r } } ^ { d a t a s e t } * W _ { c } ) + B )
$$

![](images/d81aba27a8ac2f5ec3e771dcc6c7819af6cf46fa4f2e82b1f94466d9f7bc85c7.jpg)  
Fig. 6 Architecture of LSTM (input feature data $Y$ from CNN and predicted output data $Z$ from LSTM)

![](images/f30f99746854ac3efa46b10107414d8bbe7dc01e1e67b6219e49e53e8526b258.jpg)  
Fig. 7 Architecture of CNN-LSTM

Where, the bigoted vector is $B$ , namely, the function’s intercept, which will be used to achieve a linear classifcation. Based on the pool data, the most commonly used technique is to perform max operations on each flter result and get the output value as shown below:

$$
X _ { T _ { r } } = [ m a x ( c ) ]
$$

Here are the two reasons to clarify the max pooling operations: It removes the non-maximal values and speeds up the computations of the upper layer.

# 5 Performance evaluation

The accuracy of the forecasting models was assessed by measuring the mean absolute error (MAE), root mean square error (RMSE), and mean absolute percentage error (MAPE). The MAE is a model evaluation metric that is generally associated with regression models. Each prediction error represents the diference between the actual and predicted values of the model. It can be defned as:

$$
M A E = \frac { 1 } { n } \sum _ { j = 1 } ^ { n } ( y _ { j } - \hat { y _ { j } } )
$$

where $y _ { j } , \hat { y _ { j } }$ indicates the actual and predicted values, respectively. $n$ represents the number of predictions. MAE provides equal weights to the errors, while RMSE is a quadratic measure since errors are squared before use, therefore it assigns higher weights to large errors. The formula for RMSE is as follows:

$$
R M S E = \sqrt { \frac { 1 } { n } \sum _ { j = 1 } ^ { n } ( y _ { j } - \hat { y _ { j } } ) ^ { 2 } }
$$

where $y _ { j }$ represents the $j ^ { t h }$ actual and $\hat { y _ { j } }$ represents the $j ^ { t h }$ predicted values, respectively. While $n$ shows the total number of predictions made. MAPE is similar to MAE but normalized by true observations. It shows how far the predictions of a model are from the respective actual values, on average.

$$
M A P E = \frac { 1 0 0 \% } { n } \sum _ { i = 1 } ^ { n } \frac { y _ { i } - \hat { y _ { i } } } { y _ { i } }
$$

# 6 Results and discussion

The experiments were carried out on real-time stock market data utilizing the Google Colaboratory, which uses Python 3.7 and ofers a single GPU cluster with an NVIDIA K80 GPU, $1 2 \mathrm { G B }$ of RAM, and a clock speed of $0 . 8 2 \mathrm { G H z }$ . The proposed framework was being used to deploy numerous forecasting models on live data streams from the NSE and NASDAQ stock exchanges. For univariate and multivariate time-series of selected stock prices, the results were evaluated utilizing incremental learning and Ofine-Online methods. Univariate time-series were derived from stock historical prices, whereas multivariate time-series include the EMA and VWAP along with the historical prices. In order to verify the efectiveness of the model, the models’ forecasts were compared to the actual share prices of the eight most liquid stocks listed on the NSE and NASDAQ.

A variety of statistical performance measures, including RMSE, MAE, and MAPE, are being used to test the model’s accuracy. MAE and RMSE are commonly used in fnancial analysis to measure the average gap between predicted and actual stock prices. The MAE is less biased for fnancial series with large values since it measures the average magnitude of those errors rather than taking into account the direction of the errors. This could, however, not adequately refect performance in case of large errors. RMSE is more informative when the overall impact is disproportionate to the increase in error. In contrast, MAE is more useful when the overall impact is proportional to the increase in error.

The incremental model learns from the data streams, where new data is constantly added. In this approach, initially, the model is trained with a small subset of data, and as a result, it shows a large deviation between actual and predicted stock prices. However, once the model is trained on a sufcient amount of data, the results get better. In the incremental learning process, there were 6404 instances of a one-year stock closing price recorded at 15-minute time intervals. EMA(10) and VWAP were calculated using the frst ten instances, and initial model training follows these calculations. From the $1 1 ^ { t h }$ instance onward, training and testing are conducted simultaneously. Figures 8 and 9 illustrate the outcomes of the actual versus predicted prices utilizing the incremental approach for the selected NSE and NASDAQ stocks, respectively.

![](images/673088317dc6491ef7086c21f0ff91e287687899520cb139e3970fa9edeb6492.jpg)  
Fig. 8 NSE incremental model results

Evaluations are done for both univariate and multivariate time-series. Univariate models are denoted by the abbreviation (U) along with the model name in this study, for example, univariate LSTM is denoted as LSTM(U). Table 2 compares the forecasting efectiveness of several machine learning models based on RMSE and MAE for the eight most liquid stocks listed on the Indian stock exchange NSE.

Additionally, Table 3 illustrates the outcomes for the eight most liquid stocks listed on the American stock exchange NASDAQ. On stock price time-series, the multivariate incremental model INC outperforms its univariate counterpart INC(U), demonstrating the efciency of technical indicators (EMA and VWAP) in estimating the future stock price.

Ofine-Online models used in this study include the stateof-the-art deep learning models (LSTM and its variants,

![](images/cda0415a832f60997ee89bef25ea20eecda3895486f8f87b118cd4466ba16a47.jpg)  
Fig. 9 NASDAQ incremental model results

Table 2 RMSE and MAE for the eight most liquid stocks of NSE   

<table><tr><td></td><td colspan="8">Root mean square error (RMSE)</td></tr><tr><td>Stock name</td><td>INC(U)</td><td>LSTM (U)</td><td>CNN-LSTM (U)</td><td>INC</td><td>S-LSTM</td><td>V-LSTM</td><td>B-LSTM</td><td>CNN</td></tr><tr><td>RELIANCE</td><td>83.080</td><td>54.870</td><td>21.634</td><td>16.850</td><td>11.410</td><td>12.150</td><td>7.700</td><td>10.939</td></tr><tr><td>HDFC</td><td>94.540</td><td>48.030</td><td>39.594</td><td>38.570</td><td>16.860</td><td>16.190</td><td>16.850</td><td>17.242</td></tr><tr><td>HDFCBANK</td><td>60.960</td><td>23.290</td><td>18.630</td><td>59.340</td><td>4.460</td><td>6.170</td><td>2.190</td><td>7.095</td></tr><tr><td>ICICIBANK</td><td>26.390</td><td>9.120</td><td>8.087</td><td>8.760</td><td>2.600</td><td>2.750</td><td>1.780</td><td>3.786</td></tr><tr><td>KOTAKBANK</td><td>38.320</td><td>41.310</td><td>26.294</td><td>36.200</td><td>10.210</td><td>8.920</td><td>7.450</td><td>9.691</td></tr><tr><td>ITC</td><td>8.510</td><td>2.020</td><td>1.663</td><td>1.920</td><td>0.270</td><td>0.430</td><td>0.350</td><td>0.778</td></tr><tr><td>INFY</td><td>67.040</td><td>13.340</td><td>16.494</td><td>18.110</td><td>5.330</td><td>6.180</td><td>4.390</td><td>8.028</td></tr><tr><td>TCS</td><td>112.450</td><td>53.460</td><td>37.141</td><td>54.720</td><td>15.430</td><td>12.430</td><td>11.350</td><td>12.948</td></tr><tr><td></td><td colspan="8">Mean absolute error (MAE)</td></tr><tr><td>RELIANCE</td><td>19.793</td><td>40.196</td><td>16.070</td><td>9.445</td><td>8.260</td><td>8.950</td><td>5.582</td><td>7.830</td></tr><tr><td>HDFC</td><td>23.336</td><td>32.826</td><td>28.500</td><td>13.928</td><td>10.528</td><td>10.857</td><td>9.58</td><td>11.854</td></tr><tr><td>HDFCBANK</td><td>13.800</td><td>10.580</td><td>13.379</td><td>15.010</td><td>8.293</td><td>9.175</td><td>4.545</td><td>4.697</td></tr><tr><td>ICICIBANK</td><td>6.019</td><td>6.602</td><td>6.337</td><td>4.836</td><td>1.796</td><td>1.972</td><td>1.268</td><td>2.717</td></tr><tr><td>KOTAKBANK</td><td>13.111</td><td>31.043</td><td>19.978</td><td>19.652</td><td>7.541</td><td>6.545</td><td>4.985</td><td>7.308</td></tr><tr><td>ITC</td><td>1.923</td><td>1.369</td><td>1.235</td><td>1.998</td><td>0.184</td><td>0.233</td><td>0.265</td><td>0.540</td></tr><tr><td>INFY</td><td>33.087</td><td>9.915</td><td>12.321</td><td>10.407</td><td>8.093</td><td>8.498</td><td>6.545</td><td>5.471</td></tr><tr><td>TCS</td><td>28.909</td><td>42.925</td><td>27.178</td><td>27.103</td><td>11.347</td><td>9.295</td><td>8.073</td><td>9.042</td></tr></table>

CNN, and CNN-LSTM). The specifcations of hyperparameters shared by all Ofine-Online models (epochs: 50 with early stopping; activation function: relu; optimizer: adam with learning rate 0.003; loss function: MSE; ) was fnalized through grid search, while CNN and CNN-LSTM utilize the additional parameters (flters: 64; kernel-size:2; pool-size: 3 for max-pooling;). Fig. 10 shows the outcome of the grid search for hyperparameter optimization, and the green line highlights the consolidation of parameters that lead to the minimum MAE on the training dataset. On the training data, these hyperparameters were used for the fnal model training. The train test split was in the ratio of $7 0 \% 3 0 \%$ , since May 2021, 6404 instances of NSE stocks and 8892 instances of NASDAQ stocks have been extracted. After using 10 instances for calculating EMA(10), the remaining instances were split for training and testing purposes. It begins forecasting only after it has collected enough instances for training; thus, the forecasting is reliable from the start.

Table 3 RMSE and MAE for the eight most liquid stocks of NASDAQ   
Figure  11 shows a comparison of RMSE for different models across diferent companies for both NSE and   

<table><tr><td></td><td colspan="8">Root mean square error (RMSE)</td></tr><tr><td>Stock name</td><td>INC (U)</td><td>LSTM (U)</td><td>CNN-LSTM (U)</td><td>INC</td><td>S-LSTM</td><td>V-LSTM</td><td>B-LSTM</td><td>CNN</td></tr><tr><td>AAPL</td><td>4.282</td><td>0.354</td><td>0.915</td><td>1.203</td><td>0.155</td><td>0.109</td><td>0.086</td><td>0.281</td></tr><tr><td>MSFT</td><td>7.039</td><td>0.395</td><td>2.205</td><td>1.493</td><td>0.256</td><td>0.381</td><td>0.169</td><td>0.763</td></tr><tr><td>AMZN</td><td>108.994</td><td>20.059</td><td>28.952</td><td>47.039</td><td>34.882</td><td>22.173</td><td>21.225</td><td>23.504</td></tr><tr><td>TSLA</td><td>19.306</td><td>4.746</td><td>8.599</td><td>7.970</td><td>5.941</td><td>6.861</td><td>3.742</td><td>3.860</td></tr><tr><td>GOOGL</td><td>90.697</td><td>13.381</td><td>23.371</td><td>43.205</td><td>20.201</td><td>23.570</td><td>12.794</td><td>15.526</td></tr><tr><td>NVDA</td><td>7.789</td><td>0.198</td><td>3.812</td><td>0.796</td><td>0.221</td><td>0.533</td><td>0.367</td><td>1.052</td></tr><tr><td>BRK-B</td><td>9.185</td><td>0.820</td><td>1.383</td><td>1.269</td><td>0.374</td><td>0.368</td><td>0.223</td><td>0.631</td></tr><tr><td>FB</td><td>9.394</td><td>3.190</td><td>2.385</td><td>2.380</td><td>2.061</td><td>2.240</td><td>0.860</td><td>1.348</td></tr><tr><td></td><td colspan="8">Mean Absolute Error (MAE)</td></tr><tr><td>AAPL</td><td>1.808</td><td>0.872</td><td>0.720</td><td>0.563</td><td>0.474</td><td>0.720</td><td>0.406</td><td>0.324</td></tr><tr><td>MSFT</td><td>1.391</td><td>12.400</td><td>1.808</td><td>0.796</td><td>0.004</td><td>0.003</td><td>0.002</td><td>0.751</td></tr><tr><td>AMZN</td><td>25.707</td><td>0.872</td><td>19.375</td><td>19.385</td><td>0.750</td><td>0.720</td><td>0.642</td><td>10.391</td></tr><tr><td>TSLA</td><td>4.909</td><td>0.951</td><td>5.532</td><td>3.540</td><td>0.794</td><td>0.785</td><td>0.680</td><td>2.012</td></tr><tr><td>GOOGL</td><td>24.119</td><td>11.634</td><td>16.967</td><td>22.217</td><td>18.710</td><td>16.008</td><td>9.601</td><td>10.817</td></tr><tr><td>NVDA</td><td>3.094</td><td>0.671</td><td>2.631</td><td>0.796</td><td>0.670</td><td>0.554</td><td>0.574</td><td>1.022</td></tr><tr><td>BRK-B</td><td>1.808</td><td>1.492</td><td>1.018</td><td>0.890</td><td>0.670</td><td>1.232</td><td>0.574</td><td>0.844</td></tr><tr><td>FB</td><td>1.972</td><td>2.368</td><td>1.279</td><td>0.943</td><td>2.981</td><td>1.954</td><td>0.794</td><td>0.852</td></tr></table>

![](images/5b86085c7353256e1fe8354993baaadf1d3db1ea2ef494e541d9408f93d0fd23.jpg)  
Fig. 10 The output from the tensorboard log fle for hyperparameter tunning through grid search

NASDAQ stocks using a line graph. Deep learning models outperform incremental models in terms of performance because they remember long patterns and can manage volatility and trends better. But, these models require training at the end of every trading session to be updated with the latest trends, seasonality, and sudden changes in the market. Forecasting outcomes using multivariate time-series was better, demonstrating that historical values alone cannot forecast better outcomes. Both LSTM and CNN produce good results, but B-LSTM outperforms others across all stocks on the NSE and NASDAQ in terms of low RMSE and MAE. Based on Fig. 11, it might seem that RELIANCE, HDFC, and TCS results are less accurate since their RMSEs for diferent models are on the higher side and so widely spread. However, model accuracy cannot be determined by RMSEs or MAEs since each company’s stock price ranges may difer. For example, TCS’s stock price lies in the range of 3000, while ITC has a stock price range of 200. As a result, TCS’s RMSE values for all models will be greater than ITC’s. This is because even a $1 \%$ diference in TCS pricing equals 30, whereas for ITC it’s only 2. Since Apple, Microsoft, Berkshire Hathaway, and Facebook have low stock prices, their RMSE is relatively low. The MAPE of the models should be compared on the same fnancial series for comparisons of their accuracy.

![](images/19ba5e3b4488f0edfe3cf7809f1fc6abdeb66c2a87f7ca664b6b0fbb611a2bc7.jpg)  
Fig. 11 Stocks RMSE for diferent models

MAPE is the most common measure to evaluate the model’s forecasting accuracy. Since it utilizes the percentage error and is scale-independent, it can be used to compare the model’s accuracy for the stocks in the diferent price ranges. Table 4 shows the MAPE results for the eight most liquid stocks listed on the NSE and NASDAQ, respectively. When considering univariate models, incremental linear regression is better than LSTM, while CNN-LSTM is the most efective. Figure 12 illustrates the graphical representation of Table 4 for a better interpretation of the results. LSTM on univariate time-series shows a high standard deviation on MAPE compared to other models and does not ft perfectly into the graph for NASDAQ stocks; therefore, LSTM results are removed from the NASDAQ graph. The analysis of the outcomes shows that B-LSTM is the most efective model among all, while CNN-LSTM is the most accurate univariate model for both NSE and NASDAQ stocks.

Table 4 MAPE for the eight most liquid stocks of NSE   

<table><tr><td></td><td colspan="8">NSE stocks</td></tr><tr><td>Stock name</td><td>INC(U)</td><td>LSTM(U)</td><td>CNN-LSTM(U)</td><td>INC</td><td>S-LSTM</td><td>V-LSTM</td><td>B-LSTM</td><td>CNN</td></tr><tr><td>RELIANCE</td><td>0.009</td><td>0.017</td><td>0.007</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.002</td><td>0.003</td></tr><tr><td>HDFC</td><td>0.007</td><td>0.013</td><td>0.011</td><td>0.005</td><td>0.004</td><td>0.005</td><td>0.002</td><td>0.004</td></tr><tr><td>HDFCBANK</td><td>0.009</td><td>0.007</td><td>0.009</td><td>0.010</td><td>0.003</td><td>0.004</td><td>0.001</td><td>0.003</td></tr><tr><td>ICICIBANK</td><td>0.009</td><td>0.009</td><td>0.009</td><td>0.007</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.004</td></tr><tr><td>KOTAKBANK</td><td>0.010</td><td>0.017</td><td>0.011</td><td>0.012</td><td>0.003</td><td>0.004</td><td>0.004</td><td>0.004</td></tr><tr><td>ITC</td><td>0.009</td><td>0.006</td><td>0.005</td><td>0.004</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002</td></tr><tr><td>INFY</td><td>0.020</td><td>0.006</td><td>0.007</td><td>0.006</td><td>0.003</td><td>0.004</td><td>0.003</td><td>0.003</td></tr><tr><td>TCS</td><td>0.009</td><td>0.012</td><td>0.008</td><td>0.008</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.003</td></tr><tr><td></td><td colspan="8">NASDAQ Stocks</td></tr><tr><td>AAPL</td><td>0.012</td><td>0.177</td><td>0.005</td><td>0.004</td><td>0.001</td><td>0.000</td><td>0.000</td><td>0.002</td></tr><tr><td>MSFT</td><td>0.005</td><td>0.075</td><td>0.006</td><td>0.003</td><td>0.001</td><td>0.001</td><td>0.000</td><td>0.003</td></tr><tr><td>AMZN</td><td>0.008</td><td>0.421</td><td>0.006</td><td>0.006</td><td>0.005</td><td>0.004</td><td>0.004</td><td>0.003</td></tr><tr><td>TSLA</td><td>0.007</td><td>0.377</td><td>0.006</td><td>0.004</td><td>0.004</td><td>0.005</td><td>0.003</td><td>0.004</td></tr><tr><td>GOOGL</td><td>0.010</td><td>0.344</td><td>0.006</td><td>0.008</td><td>0.004</td><td>0.006</td><td>0.003</td><td>0.004</td></tr><tr><td>NVDA</td><td>0.011</td><td>0.054</td><td>0.011</td><td>0.796</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.003</td></tr><tr><td>BRK-B</td><td>0.006</td><td>0.184</td><td>0.003</td><td>0.003</td><td>0.001</td><td>0.000</td><td>0.001</td><td>0.004</td></tr><tr><td>FB</td><td>0.006</td><td>1.028</td><td>0.004</td><td>0.003</td><td>0.008</td><td>0.005</td><td>0.002</td><td>0.002</td></tr></table>

![](images/db56e7224c44723d2c3fc41a25767eda82ee95edaffc5156de10e4561ed490db.jpg)  
Fig. 12 MAPE of diferent models on NSE and NASDAQ Stocks

As B-LSTM provides the most accurate forecasts, we have selected it for the comparison of actual versus predicted stock prices for Ofine-Online models. Figure 13 and 14 demonstrate the plot of B-LSTM for current stock prices versus the predicted values for the selected NSE and NAS-DAQ stocks. From the fgures, it is clear that the predictions are very close to the actual values for all the selected stocks, which confrms the efciency of the B-LSTM in forecasting.

models, the average RMSE of NASDAQ stocks is lower than that of NSE stocks, since NSE stocks are denominated in rupees rather than dollars, so their range is larger. Moreover, the above results also indicate that multivariate models are more accurate than univariate models. The main reason is that the multivariate model considers more than one aspect. Multivariate models consider several independent variables to help forecast stock prices more accurately. Hence, multivariate models outperformed univariate ones even with only two additional indicators: the EMA and VWAP.

The models have been evaluated using real-time trading data during operational trading hours for a $1 5 \mathrm { - m i n }$ interval. A time diference (latency) was calculated between the actual retrieval time and the forecasting time. The experimental fndings are shown in Fig. 16. As compared to the incremental model, the Ofine-Online model has a lower latency. As it does not require retraining during operational hours which results in less forecasting delay. While the incremental model gets updated on the retrieval of new instances from the live feed which requires some additional training time. The average forecasting delay for incremental learning is $9 4 0 ~ \mathrm { m s }$ , while that of Ofine-Online forecasting is $6 1 7 \mathrm { m s }$ . The forecasting delay for both approaches is less than a second, which makes them closer to real-time forecasting. Traders might fnd these models useful in making short-term trading strategies for efective trading. The Offline-Online model has the limitation that it must be retrained after each trading session to stay up-to-date with current trends. For training purposes, the model requires a signifcant number of high-frequency historical instances of stock. Thus, it might be less accurate for stocks lacking high-frequency historical data.

![](images/8e35916ddc458a7c24ee635b57d1b4b7c88e5aa02f52a1c56818ac514d20bf81.jpg)  
Figure 15 shows the comparison of the mean RMSE for all the studied models on NSE and NASDAQ stocks. For all   
Fig. 13 NSE ofine–online CNN results

# 7 Conclusion and future work

This study explores incremental and Ofine-Online learning techniques for NASDAQ and NSE stock forecasting. The models used for this study were trained on the most recent stock data while the stock’s time-series was continuously updated from the live market feed so that these models could fne-tune their hyperparameters based on the changes that occurred in the stock’s time-series during the trading sessions. A thorough analysis of various technical indicators that help in better price prediction led us to select the EMA and VWAP as features to consider along with the stock price for creating an efective multivariate time-series dataset. Furthermore, the system was tested on the top 8 stocks listed on the NSE and NASDAQ, respectively, and the performance of models was evaluated through RMSE, MAE, and MAPE. All the models forecasted better on multivariate time-series, showing the utility of the EMA and VWAP in predicting stock prices. B-LSTM was the leading performer among all for both Indian and US stocks, with the MAPE relatively close to zero. It is appropriate for short-term stock price prediction and may act as a helpful resource for the traders’ eforts to maximize the returns on intraday trading. The B-LSTM approach also provides real experience to anyone conducting research on high-frequency fnancial time-series. In the future, it may be useful to use deep learning in combination with incremental approaches to avoid Ofine-Online model retraining after each trading session. Furthermore, global sentiment can be considered one of the features of multivariate time-series.

![](images/2035f09723b0744e21c41821def597f3e5de1b4e9e652763a7f676fa532c0c83.jpg)  
Fig. 14 Nasdaq ofine–online CNN results

![](images/daa3553c47c6e8ee1f155795784b92a13d44ed393e35c05b411f297be8552d07.jpg)  
Fig. 15 Average RMSE of NSE and NASDAQ for diferent models

![](images/0ffef3284feda3280e9f7a33243e2dd445c4ba55649e5072f45c6d84ec0f78c9.jpg)  
Fig. 16 A comparison of diferent studied approaches for forecasting delay

Funding No funding was received to assist with the preparation of this manuscript.

Data availibility The datasets analyzed during this study have been web scraped through Zerodha and AlphaVantage APIs, the complete details have been discussed in Sect. 3. It will also be made available on request.

# Declarations

Conflict of interest The authors have no relevant fnancial or non-fnancial interests to disclose.

# References

Aldridge I, Krawciw S (2017) Real-time risk: What investors should know about fntech, high-frequency trading, and fash crashes. John Wiley & Sons   
Alves SA, Caarls W, Lima PM (2018) Weightless neural network for high frequency trading. In: 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1–7 . IEEE   
Analysis ATS. Partial Autocorrelation. (Accessed on 11/10/2020). https://online.stat.psu.edu/stat510/lesson/2/2.2   
Bagheri A, Peyhani HM, Akbari M (2014) Financial forecasting using anfs networks with quantum-behaved particle swarm optimization. Expert Syst Appl 41(14):6235–6250   
Behera RK, Das S, Rath SK, Misra S, Damasevicius R (2020) Comparative study of real time machine learning models for stock prediction through streaming data. J Univers Comput Sci 26(9):1128–1147   
Cao L, Tay FE (2001) Financial forecasting using support vector machines. Neural Comput Appl 10(2):184–192   
Chen CJ, Liu X, Lai KK (2013) Comparisons of strategies on gold algorithmic trading. In: 2013 Sixth International Conference on Business Intelligence and Financial Engineering, pp. 286–290 . https://doi.org/10.1109/BIFE.2013.61   
Dan J, Guo W, Shi W, Fang B, Zhang T (2014) Deterministic echo state networks based stock price forecasting. In: Abstract and Applied Analysis, vol. 2014 . Hindawi   
Du X, Zhang H, Van Nguyen H, Han Z (2017) Stacked lstm deep learning model for trafc prediction in vehicle-to-vehicle communication. In: 2017 IEEE 86th Vehicular Technology Conference (VTC-Fall), pp. 1–5 . IEEE   
González JP, San Roque AMSM, Perez EA (2017) Forecasting functional time series with a new hilbertian armax model: Application to electricity price forecasting. IEEE Trans Power Syst 33(1):545–556   
Graves A, Fernández S, Schmidhuber J (2005) Bidirectional lstm networks for improved phoneme classifcation and recognition. In: International Conference on Artifcial Neural Networks, pp. 799–804 . Springer   
Ince H, Trafalis TB (2008) Short term forecasting with support vector machines and application to stock price prediction. Int J Gen Syst 37(6):677–687   
Iscen A, Zhang J, Lazebnik S, Schmid C (2020) Memory-efcient incremental learning through feature adaptation. In: European Conference on Computer Vision, pp. 699–715 . Springer   
Javed Awan M, Mohd Rahim MS, Nobanee H, Munawar A, Yasin A, Zain AM (2021) Social media and stock market prediction: A big data approach. MJ Awan, M. Shafry, H. Nobanee, A. Munawar, A. Yasin et al.,“ Social media and stock market prediction: a big data approach,” Computers, Materials & Continua 67(2), 2569–2583   
Kim K (2003) Financial time series forecasting using support vector machines. Neurocomputing 55(1–2):307–319   
Kite Z. Kite Connect trading APIs. kite.trade. [Online; accessed 2022-06-15]   
Kumar D, Sarangi PK, Verma R (2021) A systematic review of stock market prediction using machine learning and statistical techniques. Materials Today: Proceedings   
Li Y, Zhang M, Chen C (2022) A deep-learning intelligent system incorporating data augmentation for short-term voltage stability assessment of power systems. Appl Energy 308:118347. https:// doi.org/10.1016/j.apenergy.2021.118347   
Liu FR, Ren MY, Zhai JD, Sui GQ, Zhang XY, Bing XY, Liu YL (2021) Bitcoin transaction strategy construction based on deep reinforcement learning. In: 2021 IEEE 2nd International Conference on Big Data, Artifcial Intelligence and Internet of Things Engineering (ICBAIE), pp. 180–183 . https://doi.org/10.1109/ ICBAIE52039.2021.9389965   
Liu Z, Zhu Z, Gao J, Xu C (2021) Forecast methods for time series data: a survey. IEEE Access 9:91896–91912   
Lu W, Li J, Li Y, Sun A, Wang J (2020) A cnn-lstm-based model to forecast stock prices. Complexity 2020   
Maguluri L, Ragupathy R (2020) An efficient stock market trend prediction using the real-time stock technical data and stock social media data. Int. J. Intell. Eng. Syst 13:316–332   
Menkveld AJ (2013) High frequency trading and the new market makers. Journal of fnancial Markets 16(4):712–740   
Nasdaq (2022) Global Markets Indexes and News | Nasdaq. www. nasdaq.com. [Online; accessed 2022-06-19]   
of India NSE (2001) Daily Reports. niftyindices.com. [Online; accessed 2022-06-19]   
Pellegrini S, Ruiz E, Espasa A (2011) Prediction intervals in conditionally heteroscedastic time series with stochastic components. Int J Forecast 27(2):308–319   
Rathor S, Agrawal S (2021) A robust model for domain recognition of acoustic communication using bidirectional lstm and deep neural network. Neural Comput Appl 33(17):11223–11232   
Ren S, He K, Girshick R, Sun J (2015) Faster r-cnn: Towards realtime object detection with region proposal networks. Advances in neural information processing systems 28   
Selvamuthu D, Kumar V, Mishra A (2019) Indian stock market prediction using artifcial neural networks on tick data. Financial Innovation 5(1):1–12   
Shakva A, Pokhrel A, Bhattarai A, Sitikhu P, Shakva S (2018) Realtime stock prediction using neural network. In: 2018 8th International Conference on Cloud Computing, Data Science & Engineering (Confuence), pp. 1–4 . IEEE   
Tuarob S, Wettayakorn P, Phetchai P, Traivijitkhun S, Lim S, Noraset T, Thaipisutikul T (2021) Davis: a unifed solution for data collection, analyzation, and visualization in real-time stock market prediction. Financial Innovation 7(1):1–32   
Vantage A. Free Stock APIs in JSON & Excel, Alpha Vantage. www. alphavantage.co. [Online; accessed 2022-06-15]   
Vijh M, Chandola D, Tikkiwal VA, Kumar A (2020) Stock closing price prediction using machine learning techniques. Procedia computer science 167:599–606   
Wen Y, Lin P, Nie X (2020) Research of stock price prediction based on pca-lstm model. In: IOP Conference Series: Materials Science and Engineering, vol. 790, p. 012109 . IOP Publishing   
Wu Y, Yuan M, Dong S, Lin L, Liu Y (2018) Remaining useful life estimation of engineered systems using vanilla lstm neural networks. Neurocomputing 275:167–179   
Zhang GP (2003) Time series forecasting using a hybrid arima and neural network model. Neurocomputing 50:159–175   
Zhou X, Pan Z, Hu G, Tang S, Zhao C (2018) Stock market prediction on high-frequency data using generative adversarial nets. Mathematical Problems in Engineering

Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afliations.

Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.